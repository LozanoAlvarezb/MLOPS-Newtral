{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newtral Simple RAG Example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will expose the implementation of a conversational system capable of answering questions over a list of documents.\n",
    "\n",
    "The process can be separated into the following systems:\n",
    "\n",
    "1. _Information Retrieval_ (IR): for each question **q** the IR system is in charge of finding the set of documents **D** where the answer is found.\n",
    "2. _Question Answering_ (QA): The QA system generates the answer to question **q** using the information present in the set of documents **D** .\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/claims.csv\"\n",
    "claims = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimReviewed</th>\n",
       "      <th>url</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InfoJobs está buscando, a través de llamadas telefónicas, 30 personas para trabajar de manera in...</td>\n",
       "      <td>https://www.newtral.es/estafa-infojobs-llamada-telefono-30-personas-empleo/20240401/</td>\n",
       "      <td>InfoJobs no está buscando a través de llamadas telefónicas 30 personas para “trabajar de manera ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         claimReviewed  \\\n",
       "0  InfoJobs está buscando, a través de llamadas telefónicas, 30 personas para trabajar de manera in...   \n",
       "\n",
       "                                                                                    url  \\\n",
       "0  https://www.newtral.es/estafa-infojobs-llamada-telefono-30-personas-empleo/20240401/   \n",
       "\n",
       "                                                                                               article  \n",
       "0  InfoJobs no está buscando a través de llamadas telefónicas 30 personas para “trabajar de manera ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 7712.07it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_documents = []\n",
    "for _, claim in tqdm(claims.iterrows(), total=len(claims)):\n",
    "    text = claim[\"article\"]\n",
    "    metadata = {\n",
    "        \"claimReviewed\": claim[\"claimReviewed\"],\n",
    "        \"url\": claim[\"url\"],\n",
    "    }\n",
    "    raw_documents.append({\"text\": text, \"metadata\": metadata})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first **design decision** is how to create the documents that will serve as sources for the QA system.\n",
    "\n",
    "In this case we have chosen to divide each article into segments of size less than or equal to _chunk_size_ characters (1000) with a sliding window of, at most, _chunk_overlap_ characters (200).\n",
    "\n",
    "To make the segments semantically coherent and syntactically correct, the text is recursively divided using different separators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 441.56it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for document in tqdm(raw_documents):\n",
    "    text = document[\"text\"]\n",
    "    metadata = document[\"metadata\"]\n",
    "\n",
    "    splits = split_text(text, chunk_size, chunk_overlap, separators)\n",
    "    for chunk in splits:\n",
    "        new_doc = {\"text\": chunk, \"metadata\": metadata}\n",
    "        documents.append(new_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IR system is in charge of retrieving the set of documents where the answer is found.\n",
    "\n",
    "All documents and the query are represented using dense vectors, obtained through a _SentenceTransformer_ model. The relevance that a document $d$ has for the query $q$ is given by the scalar product between both, $ddot q$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = \"data/vectorstore.npy\"\n",
    "if os.path.exists(EMBEDDINGS_PATH):\n",
    "    vectorstore = np.load(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    vectorstore = model.encode(\n",
    "        [doc[\"text\"] for doc in documents], show_progress_bar=True\n",
    "    )\n",
    "    np.save(EMBEDDINGS_PATH, vectorstore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class IR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: list[Dict[str, Any]],\n",
    "        vectorstore: np.ndarray,\n",
    "        model: SentenceTransformer,\n",
    "    ):\n",
    "        self.documents = documents\n",
    "        self.vectorstore = vectorstore\n",
    "        self.model = model\n",
    "\n",
    "    def search(self, query: str, k: int = 4):\n",
    "        \"\"\"Retrieves the top k documents from the index, by relevance to the query\"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        scores = util.dot_score(query_embedding, self.vectorstore)\n",
    "        scores = scores.squeeze()\n",
    "\n",
    "        # Bigger is better\n",
    "        topk = (-scores).argsort()[:k]\n",
    "\n",
    "        return [{**self.documents[i], \"score\": scores[i].item()} for i in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = IR(documents, vectorstore, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Any, Generator\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    @abstractmethod\n",
    "    def completion_stream(self, *args: Any, **kwarg: Any) -> Generator:\n",
    "        \"\"\"Generate text stream\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def completion(self, *args, **kwargs):\n",
    "        \"\"\"Generate text\"\"\"\n",
    "        return \"\".join(self.completion_stream(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "class ChatGPT(LLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        top_p: int = 1,\n",
    "        frequency_penalty: float = 0,\n",
    "        presence_penalty: float = 0,\n",
    "        n: int = 1,\n",
    "        logit_bias: Optional[dict] = None,\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        self.client = openai.Client()\n",
    "\n",
    "        self.params = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "            \"n\": n,\n",
    "            \"logit_bias\": logit_bias if logit_bias else {},\n",
    "        }\n",
    "\n",
    "        if seed:\n",
    "            self.params[\"seed\"] = seed\n",
    "\n",
    "    def completion_stream(self, messages):\n",
    "        stream = self.client.chat.completions.create(\n",
    "            messages=messages, stream=True, **self.params\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            token = content if content is not None else \"\"\n",
    "            yield token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute ollama code, you need to: \n",
    "\n",
    "1. Download the app and execute it in background: https://ollama.com/download/mac.\n",
    "\n",
    "2. Run the code, it may take sometime depending on your PC capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "class Ollama(LLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def completion_stream(self, messages):\n",
    "        stream = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            token = content if content is not None else \"\"\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chatgpt = ChatGPT(model=\"gpt-3.5-turbo-0125\")\n",
    "llm_ollama = Ollama(model=\"llama2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_separator = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "history_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatQA:\n",
    "    def __init__(self, ir, llm):\n",
    "        self.ir = ir\n",
    "        self.llm = llm\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "\n",
    "    def follow_up_query(self, question):\n",
    "        prompt = history_template.format(\n",
    "            chat_history=\"/n\".join(self.history), question=question\n",
    "        )\n",
    "        query = self.llm.completion(prompt)\n",
    "        return query\n",
    "\n",
    "    def __call__(self, question: str, history=False):\n",
    "        if history and len(self.history):\n",
    "            query = self.follow_up_query(question)\n",
    "        else:\n",
    "            query = question\n",
    "\n",
    "        documents = self.ir.search(query)\n",
    "\n",
    "        contexts = [document[\"text\"] for document in documents]\n",
    "        context = document_separator.join(contexts)\n",
    "        prompt = question_template.format(context=context, question=query)\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "        answer = self.llm.completion(messages)\n",
    "\n",
    "        if history:\n",
    "            self.history.append(\"\\n\".join([prompt, answer]))\n",
    "\n",
    "        urls = [document[\"metadata\"][\"url\"] for document in documents]\n",
    "        urls = list(dict.fromkeys(urls))\n",
    "        citation = [f\"{i+1}. {url}\" for i, url in enumerate(urls)]\n",
    "\n",
    "        return \"\\n\".join([answer, *citation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatQA(index, llm_chatgpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Taylor Swift se cayó de un columpio en las bahamas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Taylor Swift no se cayó de un columpio en Las Bahamas. El video que se ha viralizado no es actual y no muestra a Taylor Swift y Travis Kelce cayéndose juntos de un columpio, ya que en 2018 aún no se conocían.\n",
      "1. https://www.newtral.es/taylor-swift-columpio/20240328/\n"
     ]
    }
   ],
   "source": [
    "print(chat(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_chat():\n",
    "    chat.reset()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    chat = ChatQA(index, llm_chatgpt)\n",
    "    msg = gr.Textbox(\n",
    "        placeholder=\"Enter text and press enter, or upload an image\",\n",
    "    )\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(question, chat_history):\n",
    "        bot_message = chat(question)\n",
    "        chat_history.append((question, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(reset_chat, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
